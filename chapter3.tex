\chapter{Vector Space}

\section{Basis}

$V$ is $F$\mbox{-}vector space, $S \subset V$:
\begin{enumerate}
    \item The linear combination of $S$ is $\lag S \rag := \{ \sum_{\alpha \in S} k_{\alpha} \alpha \}$.
    \item The linear relationship can be viewed as a function $k \in F^S$. All linear relationships on set $S$ can be denoted as $\{ k \in F^S : \sum_{\alpha \in S} k_{\alpha} \alpha = 0 \}$.
    \begin{enumerate}
        \item[2.1] We say $S$ is linearly independent iff $\{ k \in F^S : \sum_{\alpha \in S} k_{\alpha} \alpha = 0 \} = \{ \calO \}$.
    \end{enumerate}
    \item $S$ is the base of $V$ iff (i) $S$ is linearly independent; (ii) $\lag S \rag = V$.
    \begin{enumerate}
        \item[3.1] $\forall v \in V$ can be uniquely denoted as $\sum_{\alpha \in S} k_{\alpha} \alpha$.
    \end{enumerate}
\end{enumerate}





\begin{proposition}
    The following propositions are equivalent:
    \begin{enumerate}
        \item $S$ is basis.
        \item $S$ is maximal linearly independent set.
        \item $S$ is minimal generating set.
    \end{enumerate}
\end{proposition}





\begin{proposition}
    The following propositions are equivalent:
    \begin{enumerate}
        \item $\{ w_1, \cdots, w_m \} \subset \lag v_1 ,\cdots, v_n \rag \land m > n$ $\Rightarrow$ $\{w_i\}_{i=1}^m$ is linearly dependent set.
        \item  $\{ w_1, \cdots, w_m \} \subset \lag v_1 ,\cdots, v_n \rag$ $\land$ $\{ w_1, \cdots, w_m \}$ is linearly independent $\Rightarrow$ $m \leq n$.
    \end{enumerate}
\end{proposition}





\begin{theorem}
    The following propositions are true:
    \begin{enumerate}
        \item Any $F$\mbl vector space has basis.
        \item Any basis of $V$ has the same cardinality.
        \item $T: V \xmapsto[]{\sim} W$ is an isomophism, then $B$ is the basis of $V$ iff $T(B)$ is the basis of $W$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Zorn's Lemma and Axiom of Choise are equivalent propositions. We use Zorn's Lemma in this proof directly. 

    (1) Let $S$ be a linearly independent set (it is permissible for$S = \emptyset$). Define set $P$ as 
    \[
        P = \{ T \subset V: S \subset T \land T \ \text{is linearly independent set} \}
    \]
    $P$, together with the subset relation $\subset$, forms a partially ordered set. Suppose $T'$ is a chain contained in $P$, let $T_0 = \bigcup_{t \in T'} t$, it can be verified that $T_0$ is linearly independent set. Thus we have established that every chain in $P$ has an upper bound. Applying Zonr's Lemma we get $P$ contains an maximal element, which is precisely the basis of $V$.

    (2) Suppose $B, B'$ are two sets of basis for $V$. We first consider the case where $|B| < \aleph_0$ and denote $B = \{ \beta_1, \cdots, \beta_n \}$.
    Since $B' \subset \lag \beta_1, \cdots, \beta_n \rag$ and $B'$ is linearly independent,  $|B'|$ must smaller than $n$. To see this, suppose for contradiction that $|B'| > n$. Then there would exists $n+1$ elements in $B'$ that also belong to $\lag \beta_1, \cdots, \beta_n \rag$, implying that these elements are linearly dependent. By a similar argument, we obtain $|B| \leq |B'|$.
    % Observe that $|B'|$ must smaller than $n$ since $B' \subset \lag \beta_1, \cdots, \beta_n \rag$ and $B'$ is linearly independent. If not, asusming that $|B'| > n$. There exists $n+1$ elements in $B'$ that also belong to $\lag \beta_1, \cdots, \beta_n \rag$, which implies these elements are dependent, contradicting the facts that $B'$ is linearly independent set. Using the same method, we obtain that $|B| \leq |B'|$.

    Now let $|B| \geq \aleph_0$, by the discussion above, we immediatly get $|B'| \geq \aleph_0$ as well. For any $\alpha \in B$, there exists a finite set $B'_{\alpha}$ that $\alpha \in \lag B'_{\alpha} \rag$. Define $A = \bigcup_{\alpha \in B} B'_{\alpha}$. It can be verified that $V = \lag A \rag$. We asser that $A = B'$. If not, there exists $\alpha' \in B' \smallsetminus A$. Notice that $\alpha' \in \lag A \rag$, so we have $\alpha' = \sum_{x \in A} k_x x$, where the equition $\alpha' - \sum_{x \in A} k_x x = 0$ is a non-trival linear relationship among the elements of $B'$, which contradicts the property of the independence of $B'$. According to proposition \ref{proposition 1.3.1}, we get:
    $$
        |B'| = \left| \bigcup_{\alpha \in B} B'_{\alpha} \right| \leq \left| \bigsqcup_{\alpha \in B} B'_{\alpha} \right| \leq \left| B \times \Z_{\geq 0} \right| = |B|
    $$

    (3) $(\Rightarrow)$: It is straitforward to verify that $T(S) \subset W$ is linearly independent and spans $W$. Furthermore, $T^{-1}$ is also a isomophism, then proof of the reverse direction follows immediatly.
\end{proof}




\begin{proposition}
    $B_i$ is set of basis of $V_i$. Let $\iota_i$ be the embedding mapping from $V_i$ to $\bigoplus^{\Ext}_{i\in I} V_i$. Then $\bigsqcup_{i\in I} \iota_i (B_i)$ is the basis of $\bigoplus^{\Ext}_{i\in I}$.
\end{proposition}





\begin{proposition}
    $V = \lag v_1, \cdots, v_m \rag$, the following propositions are true.
    \begin{enumerate}
        \item $V$ has basis.
        \item $\exists n \in Z_{\geq 0}$ such that $\dim V = n \leq m$.
        \item Any linearly independent set can be extended to basis.
        \item Any spaning set can be reduced to basis.
    \end{enumerate}
\end{proposition}








\section{Direct Sum}

\begin{definition}
    Let $(V_i)_{i \in I}$ be a series of $F \mbl$vector space:
    \begin{enumerate}
        \item $\prod_{i \in I} V_i := \left\{ \left[f: I \to \bigcup_{i \in I} V_i\right]: f(i) \in V_i \right\}$.
        \item $\bigoplus^{\Ext}_{i \in I} V_i := \left\{ (v_i)_{i\in I} \in \prod_{i \in I} V_i : \text{finite many } v_i \neq 0 \right\}$
    \end{enumerate}
    If $V_i$ is the subspace of $V$:
    \begin{enumerate}
        \item $\sum_{i \in I} V_i := \left\{ \sum_{i \in I} v_i \in V: v_i \in V_i \land \text{finite many } v_i \neq 0 \right\}$
    \end{enumerate}
\end{definition}


We define $\sigma$ as follows:
\begin{align*}
    \sigma: \bigoplus^{\Ext}_{i \in I} V_i & \to \sum_{i \in I} V_i \\
    (v_i)_{i \in I} & \mapsto \sum_{i \in I} v_i
\end{align*}
It can be verified that $\sigma$ is a well defined linear mapping, and is surjective. When $\sigma$ is injective, the vector space in both sides are isomophic, in which case we use $\bigoplus_{i \in I} V_i$ to represent $\sum_{i \in I} V_i$. Additionally, the definition of external direct sum can be approached from two perspectives, as showed in the following formula:
\[
    \bigoplus^{\Ext}_{i \in I} V_i = \bigoplus_{i \in I} \iota_i (V_i)
\]






\begin{proposition}
    The following propositions are equivalent:
    \begin{enumerate}
        \item $\sigma$ is injection.
        \item $V_i \cap \sum_{j \in I \smallsetminus \{i\}} V_j = \{ 0 \}$.
        \item Every $v \in \sum_{i \in I} V_i$ can be uniquely decomposited into the form $\sum_{i \in I} v_i$.
        \item $0 \in \sum_{i \in I} V_i$ can be only decomposited into the form $0 + \cdots$
        \item ($V_i$ is finite dimentional sapce and $I$ is finite set) $\dim(\sum_{i \in I} V_i) = \sum_{i \in I} \dim V_i$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    \[
        \begin{aligned}
            \sigma \textrm{ is injection} & \Leftrightarrow \left( \sum_{i \in I} v_i = \sum_{i \in I} w_i \Rightarrow (v_i)_{i \in I} = (w_i)_{i \in I} \right) \\
            & \Leftrightarrow \left( \sum_{i \in I} (v_i - w_i) = 0 \Rightarrow (v_i - w_i)_{i \in I} = 0\right) \\
            & \Leftrightarrow \left( \sum_{i \in I} a_i = 0 \Rightarrow (a_i)_{i \in I} = 0\right)
        \end{aligned} 
   \]
   We can extract the equivalence of the 1st, 3rd and 4th propositions from the above formula. Subsequently, we proceed to prove the equivalence between the 1st and 2nd propositions. Assuming that $\sigma$ is injective, and that $\gamma \in V_i \cap \sum_{j \in I \smallsetminus \{i\}} V_j$. It follows that $\gamma - \sum_{j \in I \smallsetminus \{i\}} v_j = 0$, thereby concluding that $\gamma = 0$. 
   For the converse, suppose that $\sum_{i \in I} v_i = 0$. Our goal is to show that $v_i = 0$. For any $i$, observe that $v_i + \sum_{j \in I \smallsetminus \{i\}} v_j \in V_i \cap \sum_{j \in I \smallsetminus \{i\}} V_j$, which implies that $v_i = 0$.

   

   In the case of $V_i$ is finite dimentional and $I$ is finite. We have
    \begin{align*}
        \sigma \textrm{ is injection} & \Leftrightarrow \bigoplus^{\Ext}_{i \in I} V_i \simeq  \sum_{i \in I} V_i  \\
        & \Leftrightarrow \dim \left( \bigoplus^{\Ext}_{i \in I} V_i \right) = \dim \left( \sum_{i \in I} V_i  \right) \\
        & \Leftrightarrow \sum_{i \in I } \dim V_i = \dim \left( \sum_{i \in I} V_i  \right)
    \end{align*}
\end{proof}






When $\sigma$ is isomophism, we define the series of functions as follows:
\begin{itemize}
    \item $\iota_i: V_i \hookrightarrow \bigoplus^{\Ext}_{j \in I} V_j : v \mapsto (v_j)_{j \in I} $ where $(v_i = v,\ v_j = 0)$.
    \item $\tilde{\iota}_i: V_i \hookrightarrow \bigoplus_{j \in I}V_j : v \mapsto v$.
    \item $p_i : \bigoplus^{\Ext}_{j \in I} V_j \to V_i:  (v_j)_{j \in I} \mapsto v_i$.
    \item $\tilde{p}_i = p_i \sigma^{-1} : \bigoplus_{j \in I}V_j \to V_i$
\end{itemize}
The diagram commutes:

\begin{center}
    \begin{tikzcd}
        &  \bigoplus^{\Ext}_{j \in I} V_j \arrow[rd, "p_i", two heads] \arrow[dd, "\sigma"', "\sim"] & \\
        V_i  \arrow[ru, "\iota_i", hook]  \arrow[rd, "\tilde{\iota}_i"', hook] & &  V_i \\
        & \bigoplus_{j \in I} V_j \arrow[ru, "\tilde{p}_i"', two heads] 
    \end{tikzcd}
\end{center}



Henceforth, we'll uniformly use use $\iota_i$ (or $p_i$) to represent either $\iota_i$ or $\tilde{\iota}_i$ ($p_i$ or $\tilde{p}_i$) in the commutative diagram above. Therefore, the function $\iota_i$ (or $p_i$) will possesses two perspectives, and under the two perspectives, it will satisfies the following properties:

\begin{corollary}
    \hfill

    \begin{enumerate}
        \item $p_i \iota_i = \idd_{V_i}$.
        \item $p_j \iota_i = \calO \ (i \neq j)$.
        \item If $I $ is finite, then $\sum_{i \in I} \iota_i p_i = \idd_{\bigoplus V_i}$
    \end{enumerate}
\end{corollary}


\begin{corollary}
    $V$ is $F\mbl$vector scpace. $P_1, \cdots, P_s \in \End(V)$ which satisfies that 
    \[
        P_1 + \cdots + P_s = \idd,\ P_i P_j = \left\{
        \begin{aligned}
            & P_i & i=j \\
            & \calO & i \neq j
        \end{aligned}
        \right.
    \]
    then the following propositions are ture:
    \begin{enumerate}
        \item $V = \bigoplus_{1 \leq i \leq s} \Im P_i$.
        \item $P_i$ is the projection form $V$ to $\Im P_i$.
    \end{enumerate}
    In particular, if $P \in \End(V)$ that satisfies $P^2 = P$, $V$ has direct sum decomposition as:
    \[
        V = \Im P_i \oplus \Im (\idd - P_i)
    \]
\end{corollary}




When $\sigma$ is isomophism and the objects in both ends of the linear mapping have direct sum decompositions, the diagram under `inner perspective' can be copied into `external perspective'. For instance, the following diagram demonstrates the copy of linear mapping $T$:

\[
    \begin{tikzcd}
        \bigoplus_{j \in J} V_j  \arrow[r, "T"]  \arrow[d, "\sigma_{1}^{-1}"', "\sim"]
        &
        \bigoplus_{i \in I} W_i \arrow[d, "\sigma_{2}^{-1}", "\sim"']
        \\
        \bigoplus_{j \in J}^{\Ext} V_j \arrow[r, "T'"'] 
        & \bigoplus_{i \in I}^{\Ext} W_i 
    \end{tikzcd}
\]
where
\[
    T' : (v_j)_{j \in J} \mapsto \left( \sum_{j \in J} p_i^{W} T \iota_j^{V} v_j \right)_{i \in I}.
\]
% Subsequently, we get a closer look of the transformation of $T$.


% If $U \subset V$ is a subspace, clearly $U = \bigoplus_{i\in I} (V_i \cap U) = \bigoplus_{i \in I} U_i$. $U_i$ is the subspace of $V$, so we can define it's external direct sum $\bigoplus^{\Ext}_{i\in I} U_i$, which can be verified is a subspace of $\bigoplus^{\Ext}_{i \in I} V_i$. 


% $V$是线性空间, $V = \bigoplus_{i \in I} V_i$, $U \subset V$ 是 $V$的一个子空间. 那么$V / U$能否分解为某些$V/U$的子空间的直和?

% 我的想法如下: 对任意$v + U \in V/U$, 其可表示为$\sum_{i \in I} (v_i + U) \in \sum_{i \in I} (V_i + U) / U$. 而$V_i + U / U$也是$V / U$的子空间. 进一步可以验证
% \[
%     V / U = \sum_{i \in I} (V_i + U) / U
% \]
% 剩下的只需证明两个不同子空间的交是零空间. 但是我无法证明$(V_i + U) / U \cap (V_j + U) / U = \{ 0 + U \}$.


% We first prove the following proposition 
Under the `external perspective' that $\iota_i$ is the embedding $V_i \hookrightarrow \bigoplus^{\Ext}_{j \in J} V_j$ and $p_i$ is the corresponding projection, the following proposition is true:


\begin{proposition}
    There exists isomophism: 
    % \begin{align*}
    %     \Hom(\bigoplus^{\Ext}_{j \in J}V_j, \prod_{i \in I}W_i) &\xrightarrow[]{\sim} \bigoplus^{\Ext}_{(i, j) \in I \times J} \Hom(V_j, W_i) \\
    \[
        \begin{tikzcd}[row sep=small]
            \Hom(\bigoplus^{\Ext}_{j \in J}V_j, \prod_{i \in I}W_i) \arrow[leftrightarrow, r, "\sim"]
            & \bigoplus^{\Ext}_{(i, j) \in I \times J} \Hom(V_j, W_i) \\
            T  \arrow[mapsto, r]
            & (T_{i,j})_{(i, j)} := (p_i^{W} T \iota_j^{V})_{(i, j)} \\
            \left[ f: (v_j)_{j \in J} \mapsto \left( \sum_{j \in J} T_{i, j} v_j \right)_{i \in I} \right]
            & (T_{i,j})_{(i, j)} \arrow[mapsto, l]
        \end{tikzcd}
    \]
    % \end{align*}
    % \[ \begin{tikzcd}[row sep=small]
	% 	\left\{ \text{子群 }  H_2 \subset G_2  \right\} \arrow[leftrightarrow, r, "1:1"] & \left\{ \text{子群 } H_1 \subset G_1 : H_1 \supset \Ker(\varphi)  \right\} \\
	% 	\left\{ \text{正规子群 }  H_2 \lhd G_2  \right\} \arrow[phantom, u, "\subset" description, sloped] \arrow[leftrightarrow, r, "1:1"] & \left\{ \text{正规子群 } H_1 \lhd G_1 : H_1 \supset \Ker(\varphi)  \right\} \arrow[phantom, u, "\subset" description, sloped] \\
	% 	H_2 \arrow[mapsto, r] & \varphi^{-1}(H_2) \\
	% 	\varphi(H_1) & H_1 \arrow[mapsto, l] .
	% \end{tikzcd} \]
\end{proposition}

In particular, let $V_j$ and $W_i$ be subspaces of $V, W$ respectively, and let $I, J$ be finite sets. We obtain that
\[
    \begin{tikzcd}
        \Hom(\bigoplus^{\Ext}_{j \in J}V_j, \bigoplus^{\Ext}_{i \in I}W_i) \arrow[leftrightarrow, r, "\sim"]
        & \bigoplus^{\Ext}_{(i, j) \in I \times J} \Hom(V_j, W_i)
    \end{tikzcd}
\]
As depicted in the previous commutative diagram, this isomophism can be copied to `inner perspective', namely: 
\[
    % \begin{tikzcd}[row sep=small]
    %     \Hom(\bigoplus^{\Ext}_{j \in J}V_j, \bigoplus^{\Ext}_{i \in I}W_i) \arrow[r, "\sim"] 
    %     & \Hom(\bigoplus_{j \in J}V_j, \bigoplus_{i \in I}W_i) \\
    %     T \arrow[mapsto, r]
    %     & \sigma_2 T\sigma^{-1}_1
    % \end{tikzcd}
    % phantom 幽灵箭头, 即不可见的箭头
    % description 可有可无
    % sloped 使标签随箭头倾斜
    \begin{tikzcd}
        T \arrow[phantom, r, "\in" description, sloped] \arrow[mapsto, d]
        & \Hom(\bigoplus^{\Ext}_{j \in J}V_j, \bigoplus^{\Ext}_{i \in I}W_i) \arrow[r, "\sim", "M"'] \arrow[d, "\sim", sloped]
        & \bigoplus^{\Ext}_{(i, j) \in I \times J} \Hom(V_j, W_i) \\
        \sigma_2 T \sigma_1^{-1} \arrow[phantom, r, "\in", sloped]
        & \Hom(\bigoplus_{j \in J}V_j, \bigoplus_{i \in I}W_i) \arrow[ru, "\sim", sloped]
        &
    \end{tikzcd}
\]

Furthermore, under the condition of compatible index set size, we can also define the ``marix multiplication'' of $(S_{i, j})_{(i, j) \in I \times J}$ and $(T_{j, k})_{(j, k) \in J \times K}$, that is:
\[
    \begin{aligned}
        \textstyle \odot : \bigoplus^{\Ext}_{(i, j) \in I \times J} \Hom(V_j, W_i) \times \bigoplus^{\Ext}_{(j, k) \in J \times K} \Hom(U_k, V_j) &\to \bigoplus^{\Ext}_{(i, K) \in I \times K} \Hom(U_k, W_i) \\
        ((S_{i, j})_{(i, j) \in I \times J}, (T_{j, k})_{(j, k) \in J \times K}) & \mapsto \left( \sum_{j \in J} S_{i, j} T_{j, k} \right)_{(i, k) \in I \times K}
    \end{aligned}
\]


\begin{proposition}
The isomophism $M$ preserves multiplication:
\[
    M(\circ(S, T)) = \odot(M(S), M(T))
\]
\end{proposition}
\begin{proof}
    $$
        \begin{aligned}
            (S\circ T)_{i, k} &= p_i^W ST \iota^U_k \\
            &= p_i^W S \left( \sum_{j \in J} \iota_j^V p_j^V \right) T \iota^U_k \\
            & = \sum_{j \in J} S_{i, j} T_{j, k}
        \end{aligned}
    $$
\end{proof}



Finlly, we use the above isomophism to derive the definition of block matrixs and their multiplication. Let the index sets, $I, J$ be finite. The conditions are listed as follows:
\begin{itemize}
    \item $V = \bigoplus_{1 \leq j \leq s} V_j,\ W = \bigoplus_{1 \leq i \leq r} W_i$.
    \item $V_j = \lag \bs{V}{j} \rag$ and $\bs{V}{j} = \{ v_{j,1}, \cdots, v_{j, n_j} \}$.
    \item $W_i = \lag \bs{W}{i} \rag$ and $\bs{W}{i} = \{ w_{i,1}, \cdots, w_{i, m_i} \}$.
    \item $n_1 + \cdots + n_s = n$ and $m_1 + \cdots + m_r = m$.
    \item $\bs{V}{1}, \cdots, \bs{V}{s}$ arranged in order form a basis $\bs{V}{}$ for $V$, and similarly for $W$, yielding a basis $\bs{W}{}$. 
\end{itemize}
Define mapping $\varphi$ as:
\[
    \begin{tikzcd}[row sep=small]
        \bigoplus_{\substack{1 \leq i \leq r \\ 1 \leq j \leq s}}\Mat_{m_i \times n_j}(F) \arrow[r, "\varphi"]
        & \Mat_{m \times n}(F) \\
        (A_{i, j})_{\substack{1 \leq i \leq r \\ 1 \leq j \leq s}} \arrow[mapsto, r]
        & \begin{bmatrix}
            A_{1, 1} & \cdots & A_{1, s} \\
            \vdots & & \vdots \\
            A_{r, 1} & \cdots & A_{r, s}
        \end{bmatrix}
    \end{tikzcd}
\]



\begin{theorem}
    The following diagram commutes:
    \[
        \begin{tikzcd}
            \Hom(\bigoplus_{j \in J}V_j, \bigoplus_{i \in I}W_i) \arrow[r, "\sim"]  \arrow[d, "\calM^{\bss{W}{}}_{\bss{V}{}}"', "\sim"]
            & \bigoplus^{\Ext}_{(i, j) \in I \times J} \Hom(V_j, W_i) \arrow[d, "\calM^{\bss{W}{i}}_{\bss{V}{j}}", "\sim"']
            \\
            \Mat_{m \times n}(F)
            & \bigoplus^{\Ext}_{(i, j) \in I \times J} \Mat_{m_i \times n_j}(F)  \arrow[l, "\varphi"', "\sim"]
        \end{tikzcd}
    \]
\end{theorem}
\begin{proof}
    \begin{equation*}
        \begin{aligned}
            T v_{j, \mu} &= \sum_{i=1}^r p_i^W \iota_i^W T \iota_j^V (v_{j, \mu}) \\
            &= \sum_{i=1}^r T_{i,j} v_{j, \mu} \\
            &= \left[ \sum_{\rho=1}^{m_1} (\calM^{\bss{W}{1}}_{\bss{V}{j}}(T_{1,j}))_{\rho, \mu} w_{1, \rho} \right] + \cdots + \left[ \sum_{\rho=1}^{m_r} (\calM^{\bss{W}{r}}_{\bss{V}{j}}(T_{r,j}))_{\rho, \mu} w_{r, \rho} \right] \\
        \end{aligned}
    \end{equation*}
    Notice that the following array is exactly the corresponding row of $\calM^{\bss{W}{}}_{\bss{V}{}}(T)$:
    \begin{equation*}
        \begin{bmatrix}
            (\calM^{\bss{W}{1}}_{\bss{V}{j}}(T_{1,j}))_{1, \mu}, (\calM^{\bss{W}{1}}_{\bss{V}{j}}(T_{1,j}))_{2, \mu}, \cdots, (\calM^{\bss{W}{r}}_{\bss{V}{j}}(T_{r,j}))_{m_r, \mu}
        \end{bmatrix}
    \end{equation*}
    Arranging them will form a matrix as:
    \begin{equation*}
        \begin{bmatrix}
            \calM^{\bss{W}{1}}_{\bss{V}{1}}(T_{1,1}) & \cdots & \calM^{\bss{W}{1}}_{\bss{V}{s}}(T_{1,s}) \\
            \vdots & & \vdots \\
            \calM^{\bss{W}{r}}_{\bss{V}{1}}(T_{r,1}) & \cdots & \calM^{\bss{W}{r}}_{\bss{V}{s}}(T_{r,s})
        \end{bmatrix}
    \end{equation*}
\end{proof}


When the index set sizes of $I, J, K$ are compatible, we can easily obtain the multiplication of blcok marixs, that is:
\begin{equation*}
    \calM((ST)_{i, k}) = \calM \left( \sum_{j \in J} S_{i, j} T_{j, k} \right) = \sum_{j \in J} \calM(S_{i, j}) \calM(T_{j, k})
\end{equation*}







\section{Linear Mapping}

The mapping $\calM$ preserves multiplication, which is represented as:
\begin{equation*}
    \calM(\circ(S, T)) = \cdot(\calM(S), \calM(T))
\end{equation*}

% When $V$ is a finite dimentional $F\mbl$vector space, one of the perspectives of $\calM(Tv)$ is to view $v$ as a linear mapping from $F$ to $V$. To be specific, for any $v \in V$ we can define a isomophism as follows:
% \[
%     \begin{tikzcd}[row sep=small]
%         V \arrow[r, "\sim"]
%         & \Hom(F, V) \\
%         v \arrow[mapsto, r]
%         & \left[ f_v: 1 \mapsto v \right] \\
%         f_v(1)
%         & f_v \arrow[mapsto, l]
%     \end{tikzcd}
% \]
% Thus $\calM(Tv) = \calM(T)\calM(v)$, where $\calM(v)$ is exactly the coordinate of $v$ under the basis that we choosen for $V$.

% Any $A \in \Mat_{m \times n}(F)$ naturally induces a linear mapping $F^n \to F^m$, defined as:
% \[
%     \begin{tikzcd}[row sep=small]
%         F^n \arrow[r]
%         & F^m
%         \\
%         (x_j)_{j=1}^n  \arrow[mapsto, r]
%         & \left( \sum_{j=1}^n a_{i, j} x_j \right)_{i=1}^m
%     \end{tikzcd}
% \]

The isomophism of the vector sapce at both ends of a linear mapping makes a copy of the mapping, represented as the diagram:
\[
    \begin{tikzcd}
        V_1  \arrow[d, "\sim"', sloped]  \arrow[r, "T"]
        & W_1 \arrow[d, "\sim", sloped]
        \\
        V_2  \arrow[r, "T'"']
        & W_2
    \end{tikzcd}
\]

Here are some common cases:
\begin{enumerate}
    \item[1.] $V, W$ are two vector spaces, $T \in \Hom(V, W)$. Under the isomophism that $V \simeq \Hom(F, V): v \mapsto [k \mapsto k \cdot v]$ the following diagram commutes, where $T \circ$ maps $f_v$ to $T \circ f_v$:
\end{enumerate}
\[
    \begin{tikzcd}
        V  \arrow[r, "T"] \arrow[d, "\sim"', sloped]
        & W \arrow[d, "\sim", sloped]
        \\
        \Hom(F, V)  \arrow[r, "T \circ"]
        & \Hom(F, W)
    \end{tikzcd}
\]
\begin{enumerate}
    \item[2.] Conctinuing the above definitions, under the isomophism between $\Hom(F, V)$ and $\Mat_{n \times 1}(F)$, the diagram commutes, where $A\cdot$ maps $x$ to $A \cdot x$:
\end{enumerate}
\[
    \begin{tikzcd}
        V  \arrow[r, "T"] \arrow[d, "\sim"', sloped]
        & W \arrow[d, "\sim", sloped]
        \\
        \Mat_{n \times 1}(F)  \arrow[r, "A \cdot"]
        & \Mat_{m \times 1}(F)
    \end{tikzcd}
\]

\begin{enumerate}
    \item[3.] Given basis $\bs{V}{}$ and $\bs{W}{}$ of $V, W$, let $\varphi_{\bss{V}{}}$ be the isomophism of $V$ and $F^n$. The diagram commutes, where $A$ maps $(x_i)_{i=1}^n$ to $(\sum_{j=1}^n a_{i,j} x_j)_{i=1}^m$:
\end{enumerate}
\[
    \begin{tikzcd}
        V  \arrow[r, "T"] \arrow[d, "\sim"', sloped]
        & W \arrow[d, "\sim", sloped]
        \\
        F^n  \arrow[r, "A"]
        & F^m
    \end{tikzcd}
\]





\begin{definition}
    $T \in \Hom(V, W)$, then $T$ is invertible iff $\exists S \in \Hom(W, V)$, such that $ST = \idd_{V} \land TS=\idd_{W}$.
\end{definition}
\begin{proposition}
    $T$ is invertible as a linear mapping iff $T$ is linear and bijective. 
\end{proposition}






\begin{proposition}
    There exists bijection that:
    \[
        \begin{tikzcd}[row sep=small]
            \{ (v_i)_{i=1}^n: \text{ordered basis} \} \arrow[leftrightarrow, r, "1:1"]
            & \{ \varphi \in \Hom(F^n, V): \text{isomophic} \} \\
            (v_i)_{i=1}^n  \arrow[mapsto, r]
            & \left[\varphi: (x_i)_{i=1}^n \mapsto \sum_{i=1}^n x_i v_i \right] \\
            (\varphi(e_i))_{i=1}^n
            & \varphi  \arrow[mapsto, l]
        \end{tikzcd}
    \]
\end{proposition}
As the isomophism of finite dimentional space and its coordinate space, $\varphi$ very commonly used.



\begin{proposition}
    Given finite dimentional $F\mbl$vector scpace $V, W$ and their basis $\bs{V}{}, \bs{W}{}$. $T \in \Hom(V, W)$. Define $\varphi_{\bss{V}{}}$ as the isomophism discussed above. Then the diagram commutes:
    \[
        \begin{tikzcd}
            V \arrow[r, "T"]
            & W
            \\
            F^n \arrow[u, "\varphi_{\bss{V}{}}"] \arrow[r, "\calM^{\bss{W}{}}_{\bss{V}{}}(T)"']
            & F^m \arrow[u, "\varphi_{\bss{W}{}}"']
        \end{tikzcd}
    \]
\end{proposition}
If $T = \idd_{V}$ and $W = V$, we also use $P^{\bss{V}{1}}_{\bss{V}{2}}$ to represent $\calM^{\bss{V}{1}}_{\bss{V}{2}}(T)$. It can be verified that 
\begin{gather*}
    P^{\bss{V}{1}}_{\bss{V}{2}} P^{\bss{V}{2}}_{\bss{V}{1}} = P^{\bss{V}{2}}_{\bss{V}{1}} P^{\bss{V}{1}}_{\bss{V}{2}} = I_n \\
    P^{\bss{V}{3}}_{\bss{V}{2}} P^{\bss{V}{2}}_{\bss{V}{1}} = P^{\bss{V}{3}}_{\bss{V}{1}}
\end{gather*}





\begin{theorem}
    Given $V, W$ and thier ordered basis $\bs{V}{1}, \bs{V}{2}, \bs{W}{1}, \bs{W}{2}$, the following diagram commutes:
    \[
        \begin{tikzcd}
            F^n \arrow[rrr, "\calM^{\bss{W}{2}}_{\bss{V}{2}}(T)"] \arrow[dd, "P^{\bss{V}{2}}_{\bss{V}{1}}"']
            &
            &
            & F^m \arrow[dd, "P^{\bss{W}{2}}_{\bss{W}{1}}"]
            \\
            & V \arrow[r, "T"] \arrow[lu, "\varphi_{\bss{V}{2}}"'] \arrow[ld, "\varphi_{\bss{V}{1}}"']
            & W \arrow[ru, "\bss{V}{2}"]  \arrow[rd, "\bss{W}{1}"]
            &
            \\
            F^n \arrow[rrr, "\calM^{\bss{W}{1}}_{\bss{V}{1}}(T)"']
            &
            &
            & F^m
        \end{tikzcd}
    \]
\end{theorem}









\section{Quotient Space}
\label{section 3.4}


\begin{theorem}
    \label{theorem 3.4.1}
    Suppose $U \subset V$ is a subspace, $T \in \Hom(V, W)$. If $U \subset \Ker T $, there exists unique $\overline{T} \in \Hom(V / U, W)$ which makes the diagram commute:
    \[
        \begin{tikzcd}
            V \arrow[two heads, r, "\pi"] \arrow[d, "T"']
            & V / \sim_U \arrow[ld, "\overline{T}"]
            \\
            W
        \end{tikzcd}
    \]
    Conctinuing with the above conditions. If $\Ker T = U$, then $\overline{T}$ is isomophism that makes the diagram commute:
    \[
        \begin{tikzcd}
            V \arrow[two heads, r, "\pi"] \arrow[d, "T"']
            & V / \sim_U \arrow[ld, "\overline{T}"] \arrow[ld, phantom, "\sim"', sloped, yshift=0.7ex]
            \\
            \Im T
        \end{tikzcd}
    \]
\end{theorem}


\begin{proof}
    using Theorem \ref{theorem equiv1} and Theorem \ref{theorem equiv2}, and prove that $\overline{T}$ is linear.
\end{proof}








\begin{theorem}
    Given a vector space $V$ endowed with a equivalence relation $\sim$, where $V / \sim$ forms a $F \mbl$vector space, and $q: V \to V/\sim$ is its quotient mapping. Then $v_1 \sim v_2 \Leftarrow v_1 - v_2 \in \Ker q \Leftarrow v_1 \sim_{\Ker q} v_2$. The mapping $\overline{q}$ derived form theorem\ref{theorem 3.4.1} is actually identity, thereby ensuring the commutativity of the diagram below:
    \[
        \begin{tikzcd}
            V \arrow[two heads, r, "\pi"] \arrow[two heads, d, "q"'] 
            & V / \Ker q \arrow[ld, "\overline{q}"] 
            \\
            V / \sim
        \end{tikzcd}
    \]
\end{theorem}










\begin{theorem}
    Suppose $U_1, U_2$ are subspaces of $V_1, V_2$ respectively. $T \in \Hom(V_1, V_2)$ satisfies $T(U_1) \subset U_2$. Then there exists $\overline{T} \in \Hom(V_1 / U_1, V_2 / U_2)$, makes the diagram commute:
    \[
        \begin{tikzcd}
            V_1 \arrow[r, "T"] \arrow[two heads, d, "\pi_1"']
            & V_2 \arrow[two heads, d, "\pi_2"]
            \\
            V_1 / U_1 \arrow[r, "\overline{T}"']
            & V_2 / U_2
        \end{tikzcd}
    \]
    Furthermore, the following diagram commutes if $T_1(U_1) \subset U_2 \land T_2(U_2) \subset U_3$:
    \[
        \begin{tikzcd}
            V_1 \arrow[r, "T_1"] \arrow[two heads, d, "\pi_1"']
            & V_2 \arrow[r, "T_2"] \arrow[two heads, d, "\pi_2"]
            & V_3 \arrow[two heads, d, "\pi_3"]
            \\
            V_1 / U_1 \arrow[r, "\overline{T}_1"']
            & V_2 / U_2 \arrow[r, "\overline{T}_2"']
            & V_3 / U_3
        \end{tikzcd}
    \]
\end{theorem}

\begin{proof}
    For the first case, $\pi_2 T$ is a linear mapping. Notice that $v \in \Ker \pi_2 T \Leftrightarrow Tv \in U_2$, thus $v \in U_1 \land T(U_1) \subset U_2 \Rightarrow v \in \Ker \pi_2 T$. Therefore $U_1 \subset \Ker \pi_2 T$, so we could apply theorem \ref{theorem 3.4.1}.
\end{proof}








\begin{theorem}[1st Isomophism Theorem]
    Suppose $V_1, V_2$ are subspace of $V$, there exists isomophism:
    \[
        \begin{tikzcd}[row sep=small]
            V_1 / (V_1 \cap V_2) \arrow[r, "\sim"]
            & (V_1 + V_2) / V_2 \\
            v_1 + (V_1 \cap V_2) \arrow[mapsto, r]
            & v_1 + V_2
        \end{tikzcd}
    \]
\end{theorem}

\begin{proof}
    $$
        \begin{tikzcd}
            V_1 \arrow[hook, r, "\iota_1"] \arrow[rr, bend left, "T"]  \arrow[d, two heads, "\pi_2"']
            & V_1 + V_2 \arrow[two heads, r, "\pi_1"]
            & (V_1 + V_2) / V_2
            \\
            V_1 / \underbracket{(V_1 \cap V_2)}_{\Ker T} \arrow[rru, "\overline{T}"']
            &
            &
        \end{tikzcd}
    $$
\end{proof}





\begin{theorem}[2nd Isomophism Theorem]
    Suppose $U \subset V$ is a subspace, then
    \begin{enumerate}
        \item[1] There exists bijection:
    \end{enumerate}
    \[
        \begin{tikzcd}[row sep=small]
            \left\{ W \subset V: \text{ subspace containing } U \right\} \arrow[leftrightarrow, r, "1:1"]
            & \left\{ \overline{W} \subset \overline{V}: \text{ subspace} \right\}
            \\
            W \arrow[mapsto, r]
            & \pi(W)
            \\
            \pi^{-1}(\overline{W})
            & \overline{W}  \arrow[mapsto, l]
        \end{tikzcd}
    \]
    \begin{enumerate}
        \item[2] $W_1 \subset W_2 \Leftrightarrow \overline{W}_1 \subset \overline{W}_2$.
        \item[3] There exists isomophism:
    \end{enumerate}
    \[
        \begin{tikzcd}[row sep=small]
            V / W \arrow[r, "\sim"]
            & \overline{V} / \overline{W}
            \\
            v + W \arrow[mapsto, r]
            & (v + U) + (V / W)
        \end{tikzcd}
    \]
\end{theorem}

\begin{proof}
    \hfill

    \begin{enumerate}
        \item[1]
        \begin{enumerate}
            \item[1.1] $\pi(W)$ is an element in prescribed set, and the mapping $W \mapsto \pi(W)$ is well defined.
            \item[1.2] $\pi^{-1}(\overline{W})$ is an element in prescribed set, and mapping $\overline{W} \mapsto \pi^{-1}(\overline{W})$ is well defined.
            \item[1.3] $W \mapsto \pi(W) \mapsto \pi^{-1}(\pi(W))$ is identity: Obviously $W \subset \pi^{-1}(\pi(W))$. For any $x \in \pi^{-1}(\pi (W))$, we have $\pi(x) \in \pi(W)$. Thus there exists $w \in W$ such that $x + U = w + U$. Hence $x - w \in U \subset W$.
            \item[1.4] $\overline{W} \mapsto \pi^{-1}(\overline{W}) \mapsto \pi(\pi^{-1} (\overline{W}))$ is identity: Obviously $\pi(\pi^{-1}(\overline{W})) \subset \overline{W}$.  Assume $\overline{x} \in \overline{W}$, by the surjection of $\pi$, there exists $x \in V$ such that $\pi(x) = \overline{x}$, because of that $\overline{W} \subset \overline{V}$. It implies $x \in \pi^{-1}(\overline{W})$, thus $\overline{x} = \pi(x) \in \pi(\pi^{-1}(\overline{W}))$.
        \end{enumerate}
    \item[2]
    \begin{enumerate}
        \item[2.1] $W_1 \subset W_2 \Rightarrow \pi(W_1) \subset \pi(W_2)$.
        \item[2.2] $\overline{W}_1 \subset \overline{W}_2 \Rightarrow \pi^{-1}(\overline{W}_1) \subset \pi^{-1}(\overline{W}_2)$.  
    \end{enumerate}
    \item[3]
    \end{enumerate}
    \[
    \begin{tikzcd}
        V \arrow[two heads, r, "\pi_1"] \arrow[two heads, d, "\pi"] \arrow[rr, bend left, "T"]
        & V/U \arrow[two heads, r, "\pi_2"]
        & (V/U) / (W/U)
        \\
        V / \underbracket{\Ker T}_{=V/W}  \arrow[rru, "\overline{T}"']
    \end{tikzcd}
    \]
\end{proof}











\begin{theorem}
    \label{theorem 3.4.6}
    Suppose $U \subset V$ is a subspace. $S_0$ is a set of basis of $U$, and $\overline{S}_1$ is a set of basis of $V / U$. Let $g \in \prod_{\overline{a} \in \overline{S}_1} \pi^{-1}(\overline{a})$, and $S_1 = g(\overline{S}_1)$. The following propositions are ture:
    \begin{enumerate}
        \item $S_1$ is linearly independent set.
        \item $\lag S_0 \rag \cap \lag S_1 \rag = \{ 0 \}$.
        \item $\lag S_0 \sqcup S_1 \rag = \lag S_0 \rag \oplus \lag S_1 \rag = V$.
        \item $\dim V = \dim U + \dim V/U$.
    \end{enumerate}
\end{theorem}



\begin{corollary}
    If $V = U \oplus W$, then $W \simeq V / U$.
\end{corollary}
\begin{proof}
    Verify that $\pi|_{W}: w \mapsto w + U$ is bijection directly.
\end{proof}



\begin{corollary}
    For any subspace $U \subset V$, there exists $W$ such that $V = U \oplus W$.
\end{corollary}




Consider the diagram under the condition that $T(U_1) \subset U_2$:
\[
    \begin{tikzcd}
        V_1  \arrow[r, "T"] \arrow[d, two heads, "\pi_1"']
        & V_2  \arrow[d ,two heads, "\pi_2"]
        \\
        V_1 / U_1  \arrow[r, "\overline{T}"']
        & V_2 / U_2
    \end{tikzcd}
\]
The other assumtions are listed as follows:
\begin{enumerate}
    \item $U_1 = \lag \bs{A}{1} \rag = \lag \alpha_1, \cdots, \alpha_r \rag$.
    \item $V_1 / U_1 = \lag \bs{A'}{2} \rag = \lag \alpha'_{r+1}, \cdots, \alpha'_{r+n} \rag$.
    \item $U_2 = \lag \bs{B}{1} \rag = \lag \beta_1, \cdots, \beta_s \rag$.
    \item $V_2 / U_2 = \lag \bs{B'}{2} \rag = \lag \beta'_{s+1}, \cdots, \beta'_{s+m} \rag$.
    \item Applying Theorem \ref{theorem 3.4.6}, we obtain the conplement space of $U_i$ and thier basis, namely $W_1 = \lag \bs{A}{2} \rag = \lag \alpha_{r+1}, \cdots, \alpha_{r+n} \rag$, $W_2 = \lag \bs{B}{2} \rag = \lag \beta_{s+1}, \cdots, \beta_{s+m} \rag$.
\end{enumerate}
Under the decomposition of $V_1 = U_1 \oplus W_1$ and $V_2 = U_2 \oplus W_2$, $\Hom(V_1, V_2)$ undergoes a decomposition, where each of its element $T$ admits an equivalent form:
\[
    \begin{bmatrix}
        T_{11} & T_{12} \\
        T_{21} & T_{22}
    \end{bmatrix}
\]
Subsequently, we proceed to prove the following propositions:
\begin{enumerate}
    \item $T_{21} = \calO$.
    \item $T_{11} = T|_{U_1}$.
    \item $\calM(T_{22}) = \calM(\overline{T})$.
\end{enumerate}

The 1st and 2nd proposition are easy to verify. Regarding the 3rd one, our inital conclusion is that the subsequent diagram commutes:
\[
    \begin{tikzcd}
        V_1 / U_1  \arrow[r, "\overline{T}"]
        & V_2 / U_2 
        \\
        W_1 \arrow[r, "\sigma_2^{-1} \overline{T} \sigma_1"'] \arrow[u, "\sigma_1 = \pi_1 |_{W_1}"]
        & W_2 \arrow[u, "\sigma_2 = \pi_2 |_{W_2}"']
    \end{tikzcd}
\]
Suppose $w_1 \in W_1$, then we have 
\[ 
    w_1 \xmapsto[]{\sigma_1} w_1 + U_1 \xmapsto[]{\overline{T}} Tw_1 + U_2 \xmapsto[]{\sigma_2^{-1}} p_2^{V_2}Tw_1 = p_2^{V_2}T \iota_{2}^{V_1}w_1 = T_{22} w_1
\]
Thus $\sigma_2^{-1} \overline{T} \sigma_1 = T_{22}$. The rest work we need to do is to demonstrate that $\calM(\sigma_2^{-1})= I_{m}, \calM(\sigma_{1}) = I_n$ and $\calM(T_{22}) = \calM(\sigma_2^{-1}) \calM(\overline{T}) \calM(\sigma_{1}) = \calM(\overline{T})$, which is straitforward to prove.

Consequently, the matix of $T$ under the prescribed basis has the form
\[
    \begin{bmatrix}
        \calM(T|_{U_1}) & * \\
        0 & \calM(\overline{T})
    \end{bmatrix}
\]

